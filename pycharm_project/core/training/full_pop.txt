inline float sigmoid(float x) {
    return 1.0f / (1.0f + exp(-x));
}

inline float derivative_sigmoid(float x) {
    return sigmoid(x) * (1 - sigmoid(x));
}

inline float derivative_relu(float x) {
    return (x > 0) ? 1.0f : 0.0f;
}

inline float clip(float value, float clip_value) {
    return fmin(fmax(value, -clip_value), clip_value);
}


__kernel void backwards(
        __global float* inputs,
        __global float* outputs_activated,
        __global float* outputs_unactivated,
        __global float* weights,
        __global float* biases,
        __global float* output_error_gradients,
        __global float* input_error_gradients_unreduced,
        __global float* weight_gradients,
        __global float* bias_gradients_unreduced,
        int input_size, int activation_type,
        float learning_rate
) {
    int input_index = get_global_id(0);
    int output_index = get_global_id(1);

    // Get the unactivated and activated output values
    float activated_value = outputs_activated[output_index];
    float unactivated_value = outputs_unactivated[output_index];

    // Compute the derivative of the activation function
    float derivative = 1.0f;
    switch (activation_type) {
        case 1: // ReLU activation
            derivative = unactivated_value > 0 ? 1.0f : 0.0f;
            break;
        case 2: // Sigmoid activation
            derivative = activated_value * (1.0f - activated_value);
            break;
        default:
            derivative = 1.0f; // Linear activation (default)
    }

    float delta = -output_error_gradients[output_index] * derivative;
    float weight_gradient = delta * inputs[input_index] * learning_rate;

    // Index calculation
    int weight_index = output_index * input_size + input_index;

    // Update gradients
    weight_gradients[weight_index] = clip(weight_gradient, 0.5f);
    bias_gradients_unreduced[output_index] = delta * learning_rate;
    input_error_gradients_unreduced[weight_index] = clip(weights[weight_index] * delta, 0.5f);
}

__kernel void reduce_input_error_gradients(
        __global float *pre_summed,    // Full array of input errors (for each input-output pair)
        __global float *summed,        // Summed error for each input node
        int input_size,                // Number of input nodes
        int output_size                // Number of output nodes
) {
    int input_index = get_global_id(0);

    float local_sum = 0.0f;
    for (int output_index = 0; output_index < output_size;output_index++) {
        int pre_summed_index = output_index * input_size + input_index;

        float value = pre_summed[pre_summed_index];
        local_sum += value;
    }

    summed[input_index] = local_sum;
}

__kernel void reduce_bias_gradients(
        __global float *pre_summed,    // Full array of input errors (for each input-output pair)
        __global float *summed,        // Summed error for each input node
        int input_size,                // Number of input nodes
        int output_size                // Number of output nodes
) {
    int output_index = get_global_id(0);

    float local_sum = 0.0f;

    for (int input_index = 0; input_index < input_size; input_index++) {
        int pre_summed_index = output_index * input_size + input_index;

        float value = pre_summed[pre_summed_index];
        local_sum += value;
    }

    // Fucked, returning NaN. So I just removed it
    //summed[output_index] = local_sum;
}